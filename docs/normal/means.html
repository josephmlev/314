<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Estimating Means</title>
  <meta name="description" content="Estimating Means">

  <link rel="canonical" href="/normal/means.html">
  <link rel="alternate" type="application/rss+xml" title="MATH 314 Lecture Notes" href="/feed.xml">

  <meta property="og:url"         content="/normal/means.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Estimating Means" />
<meta property="og:description" content="Estimating Means" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "/normal/means.html",
  "headline":
    "Estimating Means",
  "datePublished":
    "2019-08-06T09:29:55-07:00",
  "dateModified":
    "2019-08-06T09:29:55-07:00",
  "description":
    "Estimating Means",
  "author": {
    "@type": "Person",
    "name": "Edward A. Roualdes"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->



  <!-- Load the auto-generating TOC -->
  <script src="/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  

  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/"><img src="/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">MATH 314 Lecture Notes</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/intro.html"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://github.com/roualdes/314"
        >
          
          GitHub repository
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/search.html">Search</a></li>
        
      
      
        <li class="c-sidebar__divider"></li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/duniform/introduction.html"
        >
          
            1.
          
          Discrete Uniform Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/duniform/probability.html"
                >
                  
                    1.1
                  
                  Random Variables and Probability
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/bernoulli/introduction.html"
        >
          
            2.
          
          Bernoulli Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/bernoulli/proportions.html"
                >
                  
                    2.1
                  
                  Estimating Proportions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/bernoulli/summary.html"
                >
                  
                    2.2
                  
                  Summary
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/distributions/introduction.html"
        >
          
            3.
          
          Distributions
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/distributions/distributions.html"
                >
                  
                    3.1
                  
                  Distributions, Mean, and Variance
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/gamma/introduction.html"
        >
          
            4.
          
          Gamma Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/gamma/parameters.html"
                >
                  
                    4.1
                  
                  Estimating Parameters
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/normal/introduction.html"
        >
          
            5.
          
          Normal Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry c-sidebar__entry--active"
                  href="/normal/means.html"
                >
                  
                    5.1
                  
                  Estimating Means
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/bootstrap/introduction.html"
        >
          
            6.
          
          Bootstrap
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/bootstrap/confidence_intervals.html"
                >
                  
                    6.1
                  
                  Confidence Intervals
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/normal_models/introduction.html"
        >
          
            7.
          
          Normal Linear Models
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/normal_models/one_mean.html"
                >
                  
                    7.1
                  
                  One Mean
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/normal_models/simple_reg.html"
                >
                  
                    7.2
                  
                  Simple Linear Regression
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/normal_models/two_means.html"
                >
                  
                    7.3
                  
                  Two Means
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/normal_models/k_means.html"
                >
                  
                    7.4
                  
                  k Means
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/normal_models/multiple_linear_reg.html"
                >
                  
                    7.5
                  
                  Multiple Linear Regression
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/references.html"
        >
          
            8.
          
          References
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/glossary.html"
        >
          
            9.
          
          Glossary
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
      <aside class="sidebar__right">
          <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
          <nav class="onthispage">
          </nav>
      </aside>
      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">






</div>


            <div class="c-textbook__content">
              <h1 id="estimating-means">Estimating Means</h1>

<p>The first formal model in these notes happened so fast, you might have missed it.  By assuming $X_n \sim_{iid} \text{Bernoulli}(p)$, we created a single model.  This one statistical model assumed the Bernoulli distribution.  Our data consisted of multiple independent observations from the identical distribution (iid), a Bernoulli distribution with unknown population parameter $p$.</p>

<p>In this section, we change the assumed distribution to the Normal distribution.  Because the support for the Normal distribution is all real numbers, this distribution applies to data that could potentially take on any value in the real line.  We complete the section by rehearsing our use of the likelihood function as it applies to Normal data.  When, out in the real world on your own, if don’t know what model to apply, assume normality.</p>

<h2 id="density-plot">Density Plot</h2>

<p>It’s common to assume any single numerical variable data follows a Normal distribution, $ Y_n \sim \text{Normal}(\mu, \sigma)$ for $n = 1, 2, \ldots, N$.  Note that we will call this a formal model although I understand that it is difficult to separate the ideas of distribution and model at this point.  For now, let’s push forward and begin as we ought to begin any analysis, plot the data.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area hidecode">
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">update_geom_defaults</span><span class="p">(</span><span class="s2">"point"</span><span class="p">,</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">))</span><span class="w">
</span><span class="n">update_geom_defaults</span><span class="p">(</span><span class="s2">"density"</span><span class="p">,</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">))</span><span class="w">
</span><span class="n">update_geom_defaults</span><span class="p">(</span><span class="s2">"path"</span><span class="p">,</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">))</span><span class="w">
</span><span class="n">old</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">theme_set</span><span class="p">(</span><span class="n">theme_bw</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theme</span><span class="p">(</span><span class="n">text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_text</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="m">18</span><span class="p">)))</span><span class="w">
</span></code></pre></div>    </div>
  </div>

</div>

<p>Let’s return to our dataset about the Order Carnivora.  Consider the variable <code class="highlighter-rouge">BW</code>, which records birth weight in grams.  These are <strong>numeric</strong> data and it’s reasonable to assume the data came from a <strong>continuous</strong> random variable, because grams can theoretically take on any positive value in a reasonable range for birth weights from the Order Carnivora.  To plot these data we’ll use a <strong>density plot</strong>.  A common alternative plot is a histogram.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">carnivora</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/roualdes/data/master/carnivora.csv"</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">carnivora</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">BW</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/normal/means_7_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The above plot tells us that the majority of our data consist of observations below $500$ grams with a few observations showing up sporadically above $500$.  How many observations are above $500$ is not immediately clear, so let’s see if we can modify the plot above to help us visualize all of the data.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ggplot</span><span class="p">(</span><span class="n">carnivora</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">BW</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_rug</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">BW</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/normal/means_9_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<h2 id="properties-of-the-normal-distribution">Properties of the Normal Distribution</h2>

<p>If $Y \sim \text{Normal}(\mu, \sigma)$, then one can <strong>standardize</strong> the random variable $Y$ by subtracting off the mean $\mu$ and scaling by the standard deviation $\sigma$,</p>

<script type="math/tex; mode=display">Z = \frac{Y - \mu}{\sigma} \sim \text{Normal}(0, 1).</script>

<p>This linear transformation of a random variable that follows a normal distribution is so common, that the random variable $Z$ has a special name.  A random variable that follows the $\text{Normal}(0, 1)$ distribution is called <strong>standard Normal</strong>; $Z$ follows a standard Normal distribution with mean $\mu = 0$ and variance $\sigma = 1$.</p>

<p>The probability density function of the $\text{Normal}(\mu, \sigma)$ distribution is</p>

<script type="math/tex; mode=display">\text{normal}(x | \mu, \sigma) = (2\pi\sigma^2)^{-1/2} \exp{\left( \frac{-(x - \mu)^2}{2\sigma^2} \right) }.</script>

<p>A plot of the standard Normal probability density function is displayed below.  Try to change the code to help you better understand that the $\text{Normal}(\mu, \sigma)$ distribution indexes an uncountable number of distributions via $\mu$ and $\sigma$.  For each specific choice of $(\mu, \sigma)$, think of it as an instantiation of a new random variable.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-4</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="o">=</span><span class="m">101</span><span class="p">))</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">stat_function</span><span class="p">(</span><span class="n">fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dnorm</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">1</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/normal/means_12_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<h2 id="likelihood">Likelihood</h2>

<h3 id="example">Example</h3>

<p>To estimate the population mean birth weight of animals from the Order Carnivora $\mu$, we’ll assume a Normal distribution, $Y_n \sim_{iid} \text{Normal}(\mu, \sigma)$.  Find the MLE of $\mu$ and then $\sigma$.</p>

<h3 id="example-1">Example</h3>

<p>Find the MLE of $(\mu, \sigma)$ using a computer.</p>

<h2 id="assumed-normality">Assumed Normality</h2>

<p>Notice from the plot above of birth weights from the Order Carnivora, the data don’t obviously come from a Normal distribution, and yet we modeled these data with a Normal distribution.  Such an assumption doesn’t offend nearly any statistician, and yet it’s almost offensive that no statistician is bothered by this.  In this subsection, we’ll explore why statisticians are often happy to assume normality.</p>

<p>Statisticians are not often bothered by assuming Normal data, because they are trained to not think about data statically.  The randomness of data, despite the data appearing to be fixed quantities, comes from imagining that the process that produced these data could be repeated (even if it can’t).  In the case of a random sample of animals from the Order Carnivora, this would mean that you could (but wouldn’t) randomly sample a new set of data from the same population of animals from the Order Carnivora.</p>

<p>Recall that earlier in these notes, we already saw this idea.  Our operational definition of probability is the limitting relative frequency of repeating the process an infinite number of times.  We are now expanding on this idea to imagine that an entire new dataset comes from each iteration.</p>

<p>Statisticians recognize that collecting new data is unlikely to happen, but based on our operational definition of probability, this theoretical resampling is just the natural, logical extension.  Since each new dataset comes from the same population, we’d assume the Normal distribution for each new dataset.  Based on the model $Y_n \sim \text{Normal}(\mu, \sigma)$, the likelihood dictates that we use the sample mean to estimate $\mu$.  The punchline to all of this is that statisticians can prove mathematically the shape of the multiple estimates of $\mu$ that would come about based on this infinite resampling.</p>

<p>Just like we can use a computer to approximate the probability a coin flip turning up heads to be $1/2$, we can simulate the shape of multiple esimates of $\mu$ from a population of animals from the Order Carnivora.  Let’s look at the code and a plot, and then I’ll explain what’s going on.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">carnivora</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">select</span><span class="p">(</span><span class="n">BW</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">na.omit</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">pull</span><span class="p">(</span><span class="n">BW</span><span class="p">)</span><span class="w"> </span><span class="c1"># use dplyr to remove NAs and retreive vector of interest</span><span class="w">

</span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="c1"># sample size</span><span class="w">
</span><span class="n">R</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1001</span><span class="w"> </span><span class="c1"># number of resampled datasets</span><span class="w">
</span><span class="n">mus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">R</span><span class="p">)</span><span class="w"> </span><span class="c1"># preallocate!</span><span class="w">

</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nf">seq_len</span><span class="p">(</span><span class="n">R</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="c1"># resample by index</span><span class="w">
    </span><span class="n">mus</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="w"> </span><span class="c1"># index a vector with a vector and calculate mean</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mus</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="c1"># sampling distribution of multiple estimates of mu</span><span class="w">
    </span><span class="n">geom_rug</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">stat_function</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">dnorm</span><span class="p">,</span><span class="w"> 
                  </span><span class="n">args</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">),</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">N</span><span class="p">)),</span><span class="w">
                  </span><span class="n">color</span><span class="o">=</span><span class="s2">"orange"</span><span class="p">)</span><span class="w">
</span></code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/normal/means_18_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Let’s explain the code above in English, before we dive into the details of what just happened statistcally.  We are interested in birth weights from animals of the Order Carnivora, but we are not interested in missing values, encoded as <code class="highlighter-rouge">NA</code>s in R.  We use the library <code class="highlighter-rouge">dplyr</code> to <code class="highlighter-rouge">select</code> only specific variables we are currently interested in, omit the <code class="highlighter-rouge">NA</code>s, and then <code class="highlighter-rouge">pull</code> out the vector of data we want from the data frame.</p>

<p>The next paragraph (if you will) of code, stores the sample size and the number of resamples to take.  The number of resamples is analogous to how many coin flips you want to simulate.  In this case, <code class="highlighter-rouge">R</code> is specifically how many new data sets you want to simulate.  Next, we preallocate a chunk of memory to hold our <code class="highlighter-rouge">R</code> estimates of $\mu$.  Preallocation is necessary in statically typed languages and an incredibly good idea in any language that allows it.</p>

<p>The for loop iterates over a vector of integers of length <code class="highlighter-rouge">R</code>.  In each iteration, we randomly sample integers that correspond the index of specific observations in our vector of data.  Sampling by index seems like a pain right now, but it is more memory efficient and we’ll learn in the future that is the more robust solution.  Each loop creates a new vector of indices <code class="highlighter-rouge">idx</code>, that we use to index our original vector of data and then calculate the sample mean.  By the end of the for loop, we have <code class="highlighter-rouge">R</code> estimates of $\mu$.</p>

<p>The plot displays a density plot of the multiple estimates of $\mu$.  This density plot (in blue) graphically represents an estimate of the <strong>sampling distribution</strong> of the sample mean.  The sampling distribution of an estimator is the theoretical distribution for the collection of statistics one would obtain if they infinitely resampled from the population of interest.  In the scenario above, we resampled and calculated the statistic the sample mean.  Because we resampled only <code class="highlighter-rouge">R</code> times, this is an approximation of the sampling distribution for the sample mean.</p>

<p>Pay particular attention to the fact that our original data is not Normal, but the collection of multiple estimates of $\mu$ are in fact nearly Normal.  This phenomenon, named the <strong>Central Limit Theorem</strong> is a classic result of mathematical statistics.  For an arbitrary population distribution with finite variance, the sampling distribution for the sample mean will be approximately $\text{Normal}(\mu, \sigma/\sqrt{N})$.  Notice the square root of the sample size in the denominator of the standard deviation.  This says that as the sample size tends to infinity, the standard deviation of the sampling distribution will collapse on the true population mean $\mu$; when you collect all the individuals from the population mean, you will know the population mean, no more estimating.  The plot above also contains the Central Limit Theorem approximation (in orange) to the sampling distribution of the sample mean.</p>

<h2 id="sampling-distributions">Sampling Distributions</h2>

<p>Recall that a statistic is any function calculated from a set of random variables (data).  By applying the logic above, all statistics have sampling distributions.  These sampling distributions come about by imagining infinitely resampling the same population and calculating the same statistic on each new sample.</p>

<p>It takes a minute to accept this fact.  You need to keep in mind that each statistic, calculated from what you once thought was a static dataset, is now to be thought of as a single random variable.  Each statistic is a function applied to random variables.  Because the arguments to the function are random variables, the statistic is a random variable.  And, random variables follow distributions.</p>

<p>Even after you accept that every statistic should be thought of as a random variable, it still doesn’t quite help you imagine the sampling distribution for that statistic.  It’s hard to imagine sampling distributions for statistics, because we don’t know their shape.  The key points that you should keep in mind are that sampling distributions</p>

<ol>
  <li>exist,</li>
  <li>are different for each statistic, and</li>
  <li>are easiest to remember when you’ve grasped how they come about; data are not static.</li>
</ol>

<h2 id="central-limit-theorem">Central Limit Theorem</h2>

<p>A more formal definition of the Central Limit Theorem goes like this.  Assume $X_n \sim_{iid} F(\theta)$ for $n = 1, \ldots, N$ where $\mathbb{E}(X) = \mu$ and $\mathbb{V}(X) = \sigma^2 &lt; \infty$.  Let $\hat{\mu}$ denote the sample mean.  Then</p>

<script type="math/tex; mode=display">\hat{\mu}\stackrel{\cdot}{\sim} \text{Normal}\left(\mu, \frac{\mathbb{D}(X)}{\sqrt{N}} \right).</script>

<p>In English, we’d read the sampling distribution of the sample mean approaches a normal distribution with mean $\mu$ and standard deviation $\mathbb{D}(X)/\sqrt{N}$ as the sample size increases, so long as the population from which the independent data were sampled has finite variance.</p>

<p>There are a few important facts about the Central Limit Theorem above.  As stated above, the Central Limit Theorem</p>

<ol>
  <li>depends on unknown population parameters, $\mu$ and $\mathbb{D}(X)$,</li>
  <li>is an approximation of the sampling distribution of the sample mean that depends on the sample size $N$, and</li>
  <li>doesn’t tell us about other statistics than the sample mean.</li>
</ol>

<p>Notice that I didn’t claim that the finiteness of the variance of the population as an important fact.  This assumption is generally a reasonable assumption that most applied statisticians are willing to accept.</p>

<p>In most introductory statistics courses, the majority of the course material is based around the Central Limit Theorem.  The Central Limit Theorem is a mathematical theorem, with it’s own assumptions, but by focusing on it, a course is making assumptions about the student’s future use of statistics.  Using only statistics that follow the CLT limits students application of statistics.  On the other hand, statisticians maintain their jobs by showing that statistics other than the sample mean follow the Central Limit Theorem, and there’s been no shortage of papers on this topic.</p>

              <nav class="c-page__nav">
  
    
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/normal/introduction">
      〈 <span class="u-margin-right-tiny"></span> Normal Distribution
    </a>
  

  
    
    <a id="js-page__nav__next" class="c-page__nav__next" href="/bootstrap/introduction">
      Bootstrap <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
