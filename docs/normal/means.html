<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Estimating Means</title>
  <meta name="description" content="Estimating Means">

  <link rel="canonical" href="/normal/means.html">
  <link rel="alternate" type="application/rss+xml" title="MATH 314 Lecture Notes" href="/feed.xml">

  <meta property="og:url"         content="/normal/means.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Estimating Means" />
<meta property="og:description" content="Estimating Means" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "/normal/means.html",
  "headline":
    "Estimating Means",
  "datePublished":
    "2019-01-23T09:46:22-08:00",
  "dateModified":
    "2019-01-23T09:46:22-08:00",
  "description":
    "Estimating Means",
  "author": {
    "@type": "Person",
    "name": "Edward A. Roualdes"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  <script src="https://unpkg.com/nbinteract-core" async></script>

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


  <!-- Google analytics -->
  <script src="/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load custom website scripts -->
  <script src="/assets/js/scripts.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    codeCell.insertAdjacentHTML('afterend', clipboardButton(id))
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>

  <!-- Load custom user CSS and JS  -->
  <script src="/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  <script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['hub'] = hub;
      } else {
        // Create the REST params
        params = {'hub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}

// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    hubUrl = rest['hub'];
    if (hubUrl !== undefined) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);
      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      link = document.querySelectorAll("a.interact-button")[0];
      if (link !== undefined) {
          // Update the interact link URL
          var href = link.getAttribute('href');
          var hub_type = '';
          if (hub_type === 'binder') {
            // If binder links exist, we need to re-work them for jupyterhub
            if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
              // If localhost, assume we're working from a local Jupyter server and remove `/hub`
              first = [hubUrl, 'git-sync'].join('/')
            } else {
              first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
            }
            href = first + '?' + binder2Jupyterhub(href);
          } else {
            // If JupyterHub links, we only need to replace the hub url
            href = href.replace("", hubUrl);
            if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
              // Assume we're working from a local Jupyter server and remove `/hub`
              href = href.replace("/hub/user-redirect", "");
            }
          }
          link.setAttribute('href', decodeURIComponent(href));

          // Add text after interact link saying where we're launching
          hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
          document.querySelectorAll("a.interact-button")[0].insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');
      }
      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <img src="/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/>
  <h2 class="c-sidebar__title">MATH 314 Lecture Notes</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/intro"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/duniform/introduction"
        >
          
            1.
          
          Discrete Uniform Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/duniform/probability"
                >
                  
                    1.1
                  
                  Random Variables and Probability
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/bernoulli/introduction"
        >
          
            2.
          
          Bernoulli Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/bernoulli/proportions"
                >
                  
                    2.1
                  
                  Estimating Proportions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/bernoulli/summary"
                >
                  
                    2.2
                  
                  Summary
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/distributions/introduction"
        >
          
            3.
          
          Distributions
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/distributions/distributions"
                >
                  
                    3.1
                  
                  Distributions, Mean, and Variance
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/gamma/introduction"
        >
          
            4.
          
          Gamma Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/gamma/parameters"
                >
                  
                    4.1
                  
                  Estimating Parameters
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/normal/introduction"
        >
          
            5.
          
          Normal Distribution
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry c-sidebar__entry--active"
                  href="/normal/means"
                >
                  
                    5.1
                  
                  Estimating Means
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/bootstrap/introduction"
        >
          
            6.
          
          Bootstrap
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/bootstrap/confidence_intervals"
                >
                  
                    6.1
                  
                  Confidence Intervals
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/lmodels/introduction"
        >
          
            7.
          
          Linear Models
        </a>

        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/references"
        >
          
            8.
          
          References
        </a>

        
      </li>

      
    
      
      

      
      

      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/glossary"
        >
          
            9.
          
          Glossary
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/choldgraf/jupyter-book">Jupyter Book</a></p>
</nav>

      <!-- Shamelessly copied from minimal mistakes -->


<!-- TOC will only show up if it has at least one item -->


  <aside class="sidebar__right">
    <nav class="onthispage">
      <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
      <ul class="toc__menu">
  <li><a href="#density-plot">Density Plot</a></li>
  <li><a href="#properties-of-the-normal-distribution">Properties of the Normal Distribution</a></li>
  <li><a href="#likelihood">Likelihood</a>
    <ul>
      <li><a href="#example">Example</a></li>
      <li><a href="#example-1">Example</a></li>
    </ul>
  </li>
  <li><a href="#assumed-normality">Assumed Normality</a></li>
  <li><a href="#sampling-distributions">Sampling Distributions</a></li>
  <li><a href="#central-limit-theorem">Central Limit Theorem</a></li>
</ul>
    </nav>
  </aside>



      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">







<a href="https://mybinder.org/v2/gh/roualdes/314/master?filepath=content%2Fnormal%2Fmeans.ipynb"><button class="interact-button" id="interact-button-hub"><img class="interact-button-logo" src="/assets/images/logo_binder.svg" alt="Interact" />Interact</button></a>



</div>


            <div class="c-textbook__content">
              <h1 id="estimating-means">Estimating Means</h1>

<p>The first formal model in these notes happened so fast, you might have missed it.  By assuming $X_n \sim_{iid} \text{Bernoulli}(p)$, we created a single model.  This one statistical model assumed the Bernoulli distribution.  Our data consisted of multiple independent observations from the identical distribution (iid), a Bernoulli distribution with unknown population parameter $p$.</p>

<p>In this section, we change the assumed distribution to the Normal distribution.  Because the support for the Normal distribution is all real numbers, this distribution applies to data that could potentially take on any value in the real line.  We complete the section by rehearsing our use of the likelihood function as it applies to Normal data.  When, out in the real world on your own, if don’t know what model to apply, assume normality.</p>

<h2 id="density-plot">Density Plot</h2>

<p>It’s common to assume any single numerical variable data follows a Normal distribution, $ Y_n \sim \text{Normal}(\mu, \sigma)$ for $n = 1, 2, \ldots, N$.  Note that we will call this a formal model although I understand that it is difficult to separate the ideas of distribution and model at this point.  For now, let’s push forward and begin as we ought to begin any analysis, plot the data.</p>

<div class="language-R input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Let’s return to our dataset about the Order Carnivora.  Consider the variable <code class="highlighter-rouge">BW</code>, which records birth weight in grams.  These are <strong>numeric</strong> data and it’s reasonable to assume the data came from a <strong>continuous</strong> random variable, because grams can theoretically take on any positive value in a reasonable range for birth weights from the Order Carnivora.  To plot these data we’ll use a <strong>density plot</strong>.  A common alternative plot is a histogram.</p>

<div class="language-R input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">carnivora</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"~/website/app/public/data/carnivora.csv"</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">carnivora</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">BW</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p class="output output_png"><img src="../images/normal/means_6_1.png" alt="png" /></p>

<p>The above plot tells us that the majority of our data consist of observations below $500$ grams with a few observations showing up sporadically above $500$.  How many observations are above $500$ is not immediately clear, so let’s see if we can modify the plot above to help us visualize all of the data.</p>

<div class="language-R input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ggplot</span><span class="p">(</span><span class="n">carnivora</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">BW</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_rug</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">BW</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p class="output output_png"><img src="../images/normal/means_8_1.png" alt="png" /></p>

<h2 id="properties-of-the-normal-distribution">Properties of the Normal Distribution</h2>

<p>If $Y \sim \text{Normal}(\mu, \sigma)$, then one can <strong>standardize</strong> the random variable $Y$ by subtracting off the mean $\mu$ and scaling by the standard deviation $\sigma$,</p>

<script type="math/tex; mode=display">Z = \frac{Y - \mu}{\sigma} \sim \text{Normal}(0, 1).</script>

<p>This linear transformation of a random variable that follows a normal distribution is so common, that the random variable $Z$ has a special name.  A random variable that follows the $\text{Normal}(0, 1)$ distribution is called <strong>standard Normal</strong>; $Z$ follows a standard Normal distribution with mean $\mu = 0$ and variance $\sigma = 1$.</p>

<p>The probability density function of the $\text{Normal}(\mu, \sigma)$ distribution is</p>

<script type="math/tex; mode=display">\text{normal}(x | \mu, \sigma) = (2\pi\sigma^2)^{-1/2} \exp{\left( \frac{-(x - \mu)^2}{2\sigma^2} \right) }.</script>

<p>A plot of the standard Normal probability density function is displayed below.  Try to change the code to help you better understand that the $\text{Normal}(\mu, \sigma)$ distribution indexes an uncountable number of distributions via $\mu$ and $\sigma$.  For each specific choice of $(\mu, \sigma)$, think of it as an instantiation of a new random variable.</p>

<div class="language-R input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-4</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">length.out</span><span class="o">=</span><span class="m">101</span><span class="p">))</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">stat_function</span><span class="p">(</span><span class="n">fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dnorm</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p class="output output_png"><img src="../images/normal/means_11_1.png" alt="png" /></p>

<h2 id="likelihood">Likelihood</h2>

<h3 id="example">Example</h3>

<p>To estimate the population mean birth weight of animals from the Order Carnivora $\mu$, we’ll assume a Normal distribution, $Y_n \sim_{iid} \text{Normal}(\mu, \sigma)$.  Find the MLE of $\mu$ and then $\sigma$.</p>

<h3 id="example-1">Example</h3>

<p>Find the MLE of $(\mu, \sigma)$ using a computer.</p>

<h2 id="assumed-normality">Assumed Normality</h2>

<p>Notice from the plot above of birth weights from the Order Carnivora, the data don’t obviously come from a Normal distribution, and yet we modeled these data with a Normal distribution.  Such an assumption doesn’t offend nearly any statistician, and yet it’s almost offensive that no statistician is bothered by this.  In this subsection, we’ll explore why statisticians are often happy to assume normality.</p>

<p>Statisticians are not often bothered by assuming Normal data, because they are trained to not think about data statically.  The randomness of data, despite the data appearing to be fixed quantities, comes from imagining that the process that produced these data could be repeated (even if it can’t).  In the case of a random sample of animals from the Order Carnivora, this would mean that you could (but wouldn’t) randomly sample a new set of data from the same population of animals from the Order Carnivora.</p>

<p>Recall that earlier in these notes, we already saw this idea.  Our operational definition of probability is the limitting relative frequency of repeating the process an infinite number of times.  We are now expanding on this idea to imagine that an entire new dataset comes from each iteration.</p>

<p>Statisticians recognize that collecting new data is unlikely to happen, but based on our operational definition of probability, this theoretical resampling is just the natural, logical extension.  Since each new dataset comes from the same population, we’d assume the Normal distribution for each new dataset.  Based on the model $Y_n \sim \text{Normal}(\mu, \sigma)$, the likelihood dictates that we use the sample mean to estimate $\mu$.  The punchline to all of this is that statisticians can prove mathematically the shape of the multiple estimates of $\mu$ that would come about based on this infinite resampling.</p>

<p>Just like we can use a computer to approximate the probability a coin flip turning up heads to be $1/2$, we can simulate the shape of multiple esimates of $\mu$ from a population of animals from the Order Carnivora.  Let’s look at the code and a plot, and then I’ll explain what’s going on.</p>

<div class="language-R input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">carnivora</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">select</span><span class="p">(</span><span class="n">BW</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">na.omit</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
    </span><span class="n">pull</span><span class="p">(</span><span class="n">BW</span><span class="p">)</span><span class="w"> </span><span class="c1"># use dplyr to remove NAs and retreive vector of interest</span><span class="w">

</span><span class="n">N</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="c1"># sample size</span><span class="w">
</span><span class="n">R</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1001</span><span class="w"> </span><span class="c1"># number of resampled datasets</span><span class="w">
</span><span class="n">mus</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="n">R</span><span class="p">)</span><span class="w"> </span><span class="c1"># preallocate!</span><span class="w">

</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nf">seq_len</span><span class="p">(</span><span class="n">R</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="c1"># resample by index</span><span class="w">
    </span><span class="n">mus</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="w"> </span><span class="c1"># index a vector with a vector and calculate mean</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mus</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="c1"># sampling distribution of multiple estimates of mu</span><span class="w">
    </span><span class="n">geom_rug</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">stat_function</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">dnorm</span><span class="p">,</span><span class="w"> 
                  </span><span class="n">args</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">),</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">N</span><span class="p">)),</span><span class="w">
                  </span><span class="n">color</span><span class="o">=</span><span class="s2">"orange"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p class="output output_png"><img src="../images/normal/means_17_1.png" alt="png" /></p>

<p>Let’s explain the code above in English, before we dive into the details of what just happened statistcally.  We are interested in birth weights from animals of the Order Carnivora, but we are not interested in missing values, encoded as <code class="highlighter-rouge">NA</code>s in R.  We use the library <code class="highlighter-rouge">dplyr</code> to <code class="highlighter-rouge">select</code> only specific variables we are currently interested in, omit the <code class="highlighter-rouge">NA</code>s, and then <code class="highlighter-rouge">pull</code> out the vector of data we want from the data frame.</p>

<p>The next paragraph (if you will) of code, stores the sample size and the number of resamples to take.  The number of resamples is analogous to how many coin flips you want to simulate.  In this case, <code class="highlighter-rouge">R</code> is specifically how many new data sets you want to simulate.  Next, we preallocate a chunk of memory to hold our <code class="highlighter-rouge">R</code> estimates of $\mu$.  Preallocation is necessary in statically typed languages and an incredibly good idea in any language that allows it.</p>

<p>The for loop iterates over a vector of integers of length <code class="highlighter-rouge">R</code>.  In each iteration, we randomly sample integers that correspond the index of specific observations in our vector of data.  Sampling by index seems like a pain right now, but it is more memory efficient and we’ll learn in the future that is the more robust solution.  Each loop creates a new vector of indices <code class="highlighter-rouge">idx</code>, that we use to index our original vector of data and then calculate the sample mean.  By the end of the for loop, we have <code class="highlighter-rouge">R</code> estimates of $\mu$.</p>

<p>The plot displays a density plot of the multiple estimates of $\mu$.  This density plot (in blue) graphically represents an estimate of the <strong>sampling distribution</strong> of the sample mean.  The sampling distribution of an estimator is the theoretical distribution for the collection of statistics one would obtain if they infinitely resampled from the population of interest.  In the scenario above, we resampled and calculated the statistic the sample mean.  Because we resampled only <code class="highlighter-rouge">R</code> times, this is an approximation of the sampling distribution for the sample mean.</p>

<p>Pay particular attention to the fact that our original data is not Normal, but the collection of multiple estimates of $\mu$ are in fact nearly Normal.  This phenomenon, named the <strong>Central Limit Theorem</strong> is a classic result of mathematical statistics.  For an arbitrary population distribution with finite variance, the sampling distribution for the sample mean will be approximately $\text{Normal}(\mu, \sigma/\sqrt{N})$.  Notice the square root of the sample size in the denominator of the standard deviation.  This says that as the sample size tends to infinity, the standard deviation of the sampling distribution will collapse on the true population mean $\mu$; when you collect all the individuals from the population mean, you will know the population mean, no more estimating.  The plot above also contains the Central Limit Theorem approximation (in orange) to the sampling distribution of the sample mean.</p>

<h2 id="sampling-distributions">Sampling Distributions</h2>

<p>Recall that a statistic is any function calculated from a set of random variables (data).  By applying the logic above, all statistics have sampling distributions.  These sampling distributions come about by imagining infinitely resampling the same population and calculating the same statistic on each new sample.</p>

<p>It takes a minute to accept this fact.  You need to keep in mind that each statistic, calculated from what you once thought was a static dataset, is now to be thought of as a single random variable.  Each statistic is a function applied to random variables.  Because the arguments to the function are random variables, the statistic is a random variable.  And, random variables follow distributions.</p>

<p>Even after you accept that every statistic should be thought of as a random variable, it still doesn’t quite help you imagine the sampling distribution for that statistic.  It’s hard to imagine sampling distributions for statistics, because we don’t know their shape.  The key points that you should keep in mind are that sampling distributions</p>

<ol>
  <li>exist,</li>
  <li>are different for each statistic, and</li>
  <li>are easiest to remember when you’ve grasped how they come about; data are not static.</li>
</ol>

<h2 id="central-limit-theorem">Central Limit Theorem</h2>

<p>A more formal definition of the Central Limit Theorem goes like this.  Assume $X_n \sim_{iid} F(\theta)$ for $n = 1, \ldots, N$ where $\mathbb{E}(X) = \mu$ and $\mathbb{V}(X) = \sigma^2 &lt; \infty$.  Let $\hat{\mu}$ denote the sample mean.  Then</p>

<script type="math/tex; mode=display">\hat{\mu}\stackrel{\cdot}{\sim} \text{Normal}\left(\mu, \frac{\mathbb{D}(X)}{\sqrt{N}} \right).</script>

<p>In English, we’d read the sampling distribution of the sample mean approaches a normal distribution with mean $\mu$ and standard deviation $\mathbb{D}(X)/\sqrt{N}$ as the sample size increases, so long as the population from which the independent data were sampled has finite variance.</p>

<p>There are a few important facts about the Central Limit Theorem above.  As stated above, the Central Limit Theorem</p>

<ol>
  <li>depends on unknown population parameters, $\mu$ and $\mathbb{D}(X)$,</li>
  <li>is an approximation of the sampling distribution of the sample mean that depends on the sample size $N$, and</li>
  <li>doesn’t tell us about other statistics than the sample mean.</li>
</ol>

<p>Notice that I didn’t claim that the finiteness of the variance of the population as an important fact.  This assumption is generally a reasonable assumption that most applied statisticians are willing to accept.</p>

<p>In most introductory statistics courses, the majority of the course material is based around the Central Limit Theorem.  The Central Limit Theorem is a mathematical theorem, with it’s own assumptions, but by focusing on it, a course is making assumptions about the student’s future use of statistics.  Using only statistics that follow the CLT limits students application of statistics.  On the other hand, statisticians maintain their jobs by showing that statistics other than the sample mean follow the Central Limit Theorem, and there’s been no shortage of papers on this topic.</p>

              <nav class="c-page__nav">
  
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/normal/introduction">
      〈 <span class="u-margin-right-tiny"></span> Normal Distribution
    </a>
  

  
    <a id="js-page__nav__next" class="c-page__nav__next" href="/bootstrap/introduction">
      Bootstrap <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
